{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOFiPHB5tmIG"
      },
      "source": [
        "1: What are Large Language Models (LLMs) and how do they function?\n",
        "Ans: Large Language Models (LLMs) are deep learning models trained on massive text datasets to understand and generate human language.\n",
        "They use Transformer architecture and are trained using next-token prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EBmmsOMtmIH"
      },
      "source": [
        "2: Discuss the impact of LLMs on traditional software development approaches.\n",
        "\n",
        "Ans:\n",
        "- Shift from rule-based systems to prompt-based systems\n",
        "- Reduced development time\n",
        "- Rise of AI-augmented workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfbcVKEptmII"
      },
      "source": [
        "3: What are the key advantages and limitations of using LLMs in real-world applications?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Advantages: Natural language understanding enables easy human interaction, and automation improves efficiency by handling repetitive tasks.\n",
        "\n",
        "Limitations: AI can produce hallucinations (incorrect information) and may exhibit bias due to training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZXMb0cwtmII"
      },
      "source": [
        "4: Describe how different industries are being transformed by the use of LLMs. Provide examples.\n",
        "\n",
        "Ans:\n",
        "\n",
        "-Healthcare: LLMs assist in clinical documentation, patient triage, and medical research summarization. For example, tools powered by OpenAI models help doctors generate patient notes and simplify complex medical reports.\n",
        "\n",
        "-Finance: Banks use LLMs for fraud detection explanations, risk analysis, customer support chatbots, and automated report generation. Companies like JPMorgan Chase apply AI to analyze financial documents and improve compliance processes.\n",
        "\n",
        "-Education: LLMs enable personalized tutoring, automated grading, and content generation. Platforms like Khan Academy use AI assistants to provide step-by-step learning support.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Compare and contrast Langchain and LamaIndex. What unique problems does each solve?\n",
        "\n",
        "Ans: LangChain and LlamaIndex both support LLM-based applications but solve different problems: LangChain focuses on building complex, multi-step LLM workflows with memory, agents, and tool/API integration, making it ideal for chatbots and autonomous systems, while LlamaIndex specializes in data ingestion, indexing, and retrieval (RAG), enabling LLMs to efficiently search and answer questions from large document collections; in short, LangChain orchestrates logic and actions, whereas LlamaIndex optimizes data access and grounding.\n",
        "\n"
      ],
      "metadata": {
        "id": "ikyDCkJxvtcB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "mWoIufhDtmIJ",
        "outputId": "a50cbb32-7b22-4553-c0b3-e0444bbb742b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5550/1346921457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create prompt template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419  # Intentional blank docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0;34m\"api_key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0masync_api_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             }\n\u001b[0;32m-> 1071\u001b[0;31m             self.root_async_client = openai.AsyncOpenAI(\n\u001b[0m\u001b[1;32m   1072\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0masync_specific\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "#6: Implement a basic Langchain pipeline using OpenAI’s LLM to answer questions based on a user input prompt.\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Create prompt template\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Answer the following question:\\nQuestion: {question}\"\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "response = chain.invoke({\"question\": \"What is Artificial Intelligence?\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5621fkOotmIL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#7: Integrate Langchain with a third-party API (e.g., weather, news) and show how responses can be generated via LLMs.\n",
        "\n",
        "import requests\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "city = 'Delhi'\n",
        "api_key = 'your_weather_api_key'\n",
        "weather_url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric'\n",
        "weather_data = requests.get(weather_url).json()\n",
        "\n",
        "weather_description = weather_data['weather'][0]['description']\n",
        "temperature = weather_data['main']['temp']\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "prompt = f'The weather in {city} is {weather_description} with temperature {temperature}°C. Explain this in a friendly way.'\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnhSkwOZtmIM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#8: Create a LamaIndex implementation that indexes a local text file and retrieves answers from it.\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query('What is the main topic?')\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlHBbqz5tmIN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#9: Demonstrate combining Langchain with LamaIndex to create a simple document-based Q&A chatbot\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "\n",
        "question = 'What is AI?'\n",
        "context = query_engine.query(question)\n",
        "final_prompt = f'Context: {context}\\nQuestion: {question}'\n",
        "response = llm.invoke(final_prompt)\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVkGC_EZtmIO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#10: A legal firm wants to use AI to summarize large volumes of legal documents and retrieve relevant information quickly.\n",
        "#Propose a solution using Langchain and LamaIndex, and explain how it would work in practice.\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "query = 'Summarize the contract termination clause.'\n",
        "retrieved_text = query_engine.query(query)\n",
        "docs = [Document(page_content=str(retrieved_text))]\n",
        "chain = load_summarize_chain(llm, chain_type='stuff')\n",
        "summary = chain.run(docs)\n",
        "print(summary)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}